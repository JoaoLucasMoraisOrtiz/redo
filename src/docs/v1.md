Nossa hipótese central é inspirada na neurociência: o aprendizado não é o armazenamento de documentos isolados, mas a criação de ligações (arestas) entre conceitos (nós).

Propomos que, para um LLM raciocinar sobre conhecimento complexo, ele precisa de uma "memória" que reflita essa estrutura conectada. O Grafo de Conhecimento (KG) serve como esse "cérebro" externo.

Guardar Conhecimento: É o ato de criar um novo nó e conectá-lo ao grafo (nossa "3ª Via", conectando a um nó-pai ontológico, ex: (Funcao_X) -[É_UM_TIPO_DE]-> (FuncaoDeAPI)).

Atualizar Conhecimento: É o ato de "sufocar" um conceito antigo criando uma nova aresta, uma atualização não-destrutiva (ex: (API_v1) -[SUBSTITUIDA_POR]-> (API_v2)).

Isso transforma o problema de "recuperação" em um problema de "raciocínio".

3. Metodologia: Uma Arquitetura de Agentes Tripartida

Para operacionalizar essa hipótese, propomos uma arquitetura cognitiva com separação de papéis, onde o LLM atua como um "gerente" que delega tarefas a agentes especializados.

3.1. O "Gerente" LLM (O Córtex)

O usuário interage apenas com este LLM. Sua única função é a Análise de Intenção. Ele recebe a query em linguagem natural e a decompõe em uma instrução estruturada.

Input: "Na API v1 existia a rota /session?"

Output (Instrução):

{
  "acao": "recuperar",
  "entradas": ["API v1", "rota /session"],
  "intencao": "Historica" 
}


3.2. O Agente de Ingestão (O Hipocampo)

Este agente lida com acao: "guardar" e acao: "atualizar". Ele é responsável por manter a integridade do KG. Quando o LLM Gerente determina que um novo conhecimento precisa ser salvo (ex: ao ler um novo documento), este agente o insere, aplicando as regras de conexão ontológica (a "3ª Via") e sufocamento (atualização não-destrutiva) que discutimos.

3.3. O Agente de Recuperação (A Alma do Raciocínio)

Este agente lida com acao: "recuperar" e é o núcleo da nossa proposta. Ele executa um algoritmo de travessia de grafo guloso, guiado por um Modelo de Ponderação.

Fluxo de Raciocínio:

Pontos de Entrada: O Agente usa a instrucao.entradas ("API v1") para fazer uma busca vetorial simples, encontrando os nós iniciais no grafo.

Travessia Ponderada: O Agente inicia um loop de busca a partir dos nós de entrada. A cada passo, ele se encontra em um Nó Atual e avalia todas as arestas e nós vizinhos ("universo de possibilidades").

O Modelo de Ponderação: Para cada caminho possível, ele consulta um modelo de ponderação, que calcula um score.

É este modelo que resolve o paradoxo da API v1:

Query: "Na API v1 existia a rota /session?" (Intencao: "Historica")

Agente está em: (API_v1)

Modelo de Ponderação avalia Caminho A: -(SUBSTITUIDA_POR)-> (API_v2)

Input do Modelo: (Query: "...", Intencao: "Historica", No_Atual: "API_v1", Caminho: "SUBSTITUIDA_POR API_v2")

Output (Score): -0.9 (O modelo aprendeu que Historica + SUBSTITUIDA_POR é uma péssima combinação).

Modelo de Ponderação avalia Caminho B: -(TEM_ROTA)-> (/session)

Input do Modelo: (Query: "...", Intencao: "Historica", No_Atual: "API_v1", Caminho: "TEM_ROTA /session")

Output (Score): 0.95 (Corresponde perfeitamente às entidades da query e respeita a intenção).

Decisão Gulosa: O Agente segue o Caminho B, entregando a resposta correta e "ignorando" o sufocamento.

4. Treinamento do Modelo de Ponderação

A eficácia do Agente de Recuperação depende inteiramente do Modelo de Ponderação. Propomos uma arquitetura baseada em Cross-Encoder, treinada com aprendizado por contraste.

Modelo: Um Cross-Encoder (ex: MiniLM ou ms-marco-bert-base).

Estratégia de Treino: Aprendizado por Contraste (Triplet Loss). Esta abordagem é superior à regressão de score, pois a geração de dados de treino é mais simples e robusta.

Geração de Dados de Treino:

Modelo Base (Global): Iniciamos com um Cross-Encoder já pré-treinado no MS MARCO, que já entende o "ranking de relevância" global.

Geração de Tríades (Domínio Específico): Usamos um LLM Gerador (ex: o modelo deepseek R1 rodando localmente via mlStudio) para processar corpora de código (vamos baixar o dataset posteriormente do hugging-face). O LLM lê tperguntas e respostas e extrai tríades de alta qualidade.

Exemplo de Tríade (Âncora, Positivo, Negativo):

Âncora (Bússola): (Query: "Como uso a API X?", Intencao: "Implementação Atual", No: "API_v1")

Positivo (Caminho Bom): (Aresta: "SUBSTITUIDA_POR", No: "API_v2")

Negativo (Caminho Ruim): (Aresta: "DOCUMENTADO_EM", No: "doc_v1.pdf")

Fine-Tuning: O modelo base (MS MARCO) é então refinado neste dataset de tríades de código.